{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pdfplumber\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "# Function to preprocess text (already defined)\n",
        "def preprocess_text(text):\n",
        "     text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace and newlines\n",
        "     text = text.strip()\n",
        "     text = text.lower()  # Convert to lowercase\n",
        "  # Optionally add more specific cleaning steps for special characters if needed\n",
        "     return text\n",
        "\n",
        "# Function to tokenize sentences from text (already defined)\n",
        "def tokenize_sentences(text):\n",
        "  text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace and newlines\n",
        "  text = text.strip()\n",
        "  text = text.lower()  # Convert to lowercase\n",
        "  # Optionally add more specific cleaning steps for special characters if needed\n",
        "  return text\n",
        "    # ...\n",
        "\n",
        "# Function to convert PDF to tokenized sentences (already defined)\n",
        "def convert_pdf_to_tokens(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "     raise ValueError(f\"PDF file not found: \")\n",
        "\n",
        "    try:\n",
        "     with pdfplumber.open(file_path) as pdf:\n",
        "      text = \"\"\n",
        "      for page in pdf.pages:\n",
        "        text += page.extract_text()\n",
        "    except Exception as e:\n",
        "     raise ValueError(f\"Error extracting text from PDF: {e}\")\n",
        "\n",
        "    return tokenize_sentences(text)\n",
        "    # ...\n",
        "\n",
        "# Function to calculate sentence similarity (already defined)\n",
        "def calculate_similarity(query_sentence, document_sentences):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    sentence = query_sentiment.lower()  # Lowercase for consistency\n",
        "\n",
        "# Encode the example sentence (no change)\n",
        "    encoded_sentence = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      sentence_embedding = model(**encoded_sentence).pooler_output\n",
        "      document_embeddings = [model(**doc_sent).pooler_output for doc_sent in encoded_document_sentences ]\n",
        "\n",
        "    similarities = [torch.nn.functional.cosine_similarity(sentence_embedding, doc_embedding) for doc_embedding in document_embeddings]\n",
        "    # ...\n",
        "\n",
        "# Function to query sentiment analysis API (implement this)\n",
        "def query_sentiment(sentence):\n",
        "\n",
        "    API_URL = \"https://api-inference.huggingface.co/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english\"  # Replace with actual URL\n",
        "    headers = {\"Authorization\": \"Bearer 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\"}  # Replace with your API key\n",
        "    payload = {\"sentence\": sentence}\n",
        "\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    sentiment_score = response.json()\n",
        "\n",
        "    return sentiment_score\n",
        "\n",
        "# Function to process PDF and analyze sentiment (example usage)\n",
        "def process_pdf_and_analyze(pdf_path, query_sentence):\n",
        "    tokens = convert_pdf_to_tokens(pdf_path)\n",
        "    document_sentences = [t.cpu() for t in tokens]  # Convert tensors to lists\n",
        "\n",
        "    similarities = calculate_similarity(query_sentence, document_sentences)\n",
        "    most_similar_idx = torch.argmax(torch.tensor(similarities))\n",
        "    most_similar_text = document_sentences[most_similar_idx]\n",
        "\n",
        "    sentiment_score =query_sentiment(query_sentence)\n",
        "    print(sentiment_score)\n",
        "\n",
        "    # Process and return results (e.g., most similar sentence, sentiment score)\n",
        "    # ...\n",
        "\n",
        "# Example usage (assuming this code is in your function app)\n",
        "def main(req: func.HttpRequest) -> func.HttpResponse:\n",
        "    \"\"\"Analyzes a PDF document based on a user query.\n",
        "\n",
        "    Args:\n",
        "        req: The HTTP request object containing query parameters.\n",
        "\n",
        "    Returns:\n",
        "        An HTTP response with the analysis results in JSON format\n",
        "        or an error message if parameters are missing or processing fails.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get parameters from the request\n",
        "    pdf_path = req.params.get(\"pdf_path\")\n",
        "    query_sentence = req.params.get(\"query_sentence\")\n",
        "\n",
        "    # Check for missing parameters\n",
        "    if not pdf_path or not query_sentence:\n",
        "        return func.HttpResponse(\n",
        "            status_code=400,\n",
        "            body=\"Missing required parameters: pdf_path and query_sentence\"\n",
        "        )\n",
        "\n",
        "    # Implement secure PDF access and processing logic here (replace with actual logic)\n",
        "    try:\n",
        "        # Access and process the PDF document based on pdf_path\n",
        "        # ... (code for processing the PDF and analyzing with query_sentence)\n",
        "        results = process_pdf_and_analyze(pdf_path, query_sentence)\n",
        "    except Exception as e:\n",
        "        # Handle potential errors during processing\n",
        "        return func.HttpResponse(\n",
        "            status_code=500,\n",
        "            body=f\"An error occurred while processing the PDF: {str(e)}\"\n",
        "        )\n",
        "\n",
        "    # Return analysis results as JSON\n",
        "    return func.HttpResponse(\n",
        "        status_code=200,\n",
        "        body=json.dumps(results)\n",
        "    )\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 43 (3094940447.py, line 47)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 47\u001b[0;36m\u001b[0m\n\u001b[0;31m    def query_sentiment(sentence):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 43\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1713352435971
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}